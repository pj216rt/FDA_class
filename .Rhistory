scale_linetype_discrete(name = expression(lambda)) +
#Adapt title for various Basis functions used
labs(title = sprintf("Penalized Fourier LS: True vs Predictions (K = %d)", K),
x = "t", y = "f(t)") +
#use the minimal theme with a larger text size
theme_minimal(base_size = 13) +
#make legend key wider.  Might help since we use different linetypes
theme(legend.title = element_text(),
legend.key.width = unit(1.6, "lines"))
estimation.plot
K <- 5
lambda.grid <- seq(0, 0.0675, length.out = 10)
Phi <- design_matrix(t_points, K)
P <- fourier_penalty(K=K)
#the dense predictions grid
t_dense <- seq(0, 1, length.out = 5*n)
Phi_dense <- design_matrix(t_dense, K)
y_true_dense <- f1(t_dense)
#precompute
XtX <- crossprod(Phi)
Xty <- crossprod(Phi, y_values)
#estimate the function for the various values of lambda
fits <- lapply(lambda.grid, function(lam) {
A <- XtX + lam * P
c_hat <- solve(A, Xty)
y_hat <- as.vector(Phi_dense %*% c_hat)
data.frame(t = t_dense,
predicted = y_hat,
lambda = lam,
lambda_lab = format(lam, scientific = TRUE, digits = 2),
stringsAsFactors = FALSE)
})
#bind the rows and turns the lambda into labels via scientific notation out to 2 digits
df_pred <- bind_rows(fits) %>%
mutate(lambda_lab = factor(format(lambda, scientific = TRUE, digits = 2),
levels = unique(format(lambda.grid, scientific = TRUE, digits = 2))))
df_true   <- data.frame(t = t_dense, true = y_true_dense)
df_points <- data.frame(t = t_points, y = y_values)
estimation.plot <- ggplot() +
#draw the true function as a black line
geom_line(data = df_true, aes(x = t, y = true), linewidth = 1.2, color = "black") +
#add a seperate linetype, color for each lambda
geom_line(data = df_pred,
aes(x = t, y = predicted, color = lambda_lab, linetype = lambda_lab),
linewidth = 0.9, alpha = 0.9) +
#plot observed data points as big points, but remove aesthetics from the previous layers
geom_point(data = df_points, aes(x = t, y = y), size = 2, alpha = 0.8, inherit.aes = FALSE) +
#Name legend
scale_color_discrete(name = expression(lambda)) +
scale_linetype_discrete(name = expression(lambda)) +
#Adapt title for various Basis functions used
labs(title = sprintf("Penalized Fourier LS: True vs Predictions (K = %d)", K),
x = "t", y = "f(t)") +
#use the minimal theme with a larger text size
theme_minimal(base_size = 13) +
#make legend key wider.  Might help since we use different linetypes
theme(legend.title = element_text(),
legend.key.width = unit(1.6, "lines"))
estimation.plot
#define a function
f1 <- function(t){
2*sin(2*pi*t) + cos(4*pi*t)
}
n <- 10
t_points <- seq(0, 1, length.out = n)
y_values <- sapply(t_points, f1)
#function to create a design matrix for a given Fourier Basis, assuming the 3
#Fourier basis functions provided, asuming K = odd >= 3
design_matrix <- function(t, K){
t <- as.numeric(t)
n <- length(t)
#number of sine/cosine pairs (subtract the intercept and divide by 2)
#if K = 3, then there is 1, K=5, then there is two, etc
J <- (K - 1) %/% 2
X <- matrix(0, nrow = n, ncol = K)
#first Fourier basis function is 1
X[,1] <- 1
col <- 2
#fills sin/cosine columns of Fourier design matrix one at a time
#J = # of pairs to include
for (k in 1:J) {
X[, col]   <- sqrt(2)*sin(2*pi*k*t)
col <- col + 1
X[, col]   <- sqrt(2)*cos(2*pi*k*t)
col <- col + 1
}
return(X)
}
#penalty matrix assuming quadtratic penalty and K ?>= 3
fourier_penalty <- function(K, order = 2) {
#number of sine/cosine pairs (subtract the intercept and divide by 2)
#if K = 3, then there is 1, K=5, then there is two, etc
J <- (K - 1) %/% 2
P <- diag(0,K)
#intercept is unpenalized
#iterate of sin/cos pairs
for (j in 1:J){
#term for each "block"
term <- (2*pi*j)^4
#sin term
#need the 2j-1 term or else the sin term goes in the cosine slot
P[1+(2*j - 1), 1+(2*j - 1)] <- term
#cosine term
#don't need the -1 addeded here
P[1+(2*j), 1+(2*j)] <- term
}
return(P)
}
#part b and c
#definding basis functions, creating design matrix and penalty matrix
K <- 5
lambda.grid <- seq(0, 0.0675, length.out = 10)
Phi <- design_matrix(t_points, K)
P <- fourier_penalty(K=K)
#the dense predictions grid
t_dense <- seq(0, 1, length.out = 5*n)
Phi_dense <- design_matrix(t_dense, K)
y_true_dense <- f1(t_dense)
#precompute
XtX <- crossprod(Phi)
Xty <- crossprod(Phi, y_values)
#estimate the function for the various values of lambda
fits <- lapply(lambda.grid, function(lam) {
A <- XtX + (lam*P)
c_hat <- solve(A, Xty)
y_hat <- as.vector(Phi_dense %*% c_hat)
data.frame(t = t_dense,
predicted = y_hat,
lambda = lam,
lambda_lab = format(lam, scientific = TRUE, digits = 2),
stringsAsFactors = FALSE)
})
#bind the rows and turns the lambda into labels via scientific notation out to 2 digits
df_pred <- bind_rows(fits) %>%
mutate(lambda_lab = factor(format(lambda, scientific = TRUE, digits = 2),
levels = unique(format(lambda.grid, scientific = TRUE, digits = 2))))
df_true   <- data.frame(t = t_dense, true = y_true_dense)
df_points <- data.frame(t = t_points, y = y_values)
estimation.plot <- ggplot() +
#draw the true function as a black line
geom_line(data = df_true, aes(x = t, y = true), linewidth = 1.2, color = "black") +
#add a seperate linetype, color for each lambda
geom_line(data = df_pred,
aes(x = t, y = predicted, color = lambda_lab, linetype = lambda_lab),
linewidth = 0.9, alpha = 0.9) +
#plot observed data points as big points, but remove aesthetics from the previous layers
geom_point(data = df_points, aes(x = t, y = y), size = 2, alpha = 0.8, inherit.aes = FALSE) +
#Name legend
scale_color_discrete(name = expression(lambda)) +
scale_linetype_discrete(name = expression(lambda)) +
#Adapt title for various Basis functions used
labs(title = sprintf("Penalized Fourier LS: True vs Predictions (K = %d)", K),
x = "t", y = "f(t)") +
#use the minimal theme with a larger text size
theme_minimal(base_size = 13) +
#make legend key wider.  Might help since we use different linetypes
theme(legend.title = element_text(),
legend.key.width = unit(1.6, "lines"))
estimation.plot
library(R.matlab)
library(tidyverse)
library(R.matlab)
#Problem 3
#we are asked to write a computer program to sample a function f with a
#given analytical form at some given time points in [0,1].
#part a
#define a function
f1 <- function(t){
2*sin(2*pi*t) + cos(4*pi*t)
}
n <- 10
t_points <- seq(0, 1, length.out = n)
y_values <- sapply(t_points, f1)
#function to create a design matrix for a given Fourier Basis, assuming the 3
#Fourier basis functions provided, asuming K = odd >= 3
design_matrix <- function(t, K){
t <- as.numeric(t)
n <- length(t)
#number of sine/cosine pairs (subtract the intercept and divide by 2)
#if K = 3, then there is 1, K=5, then there is two, etc
J <- (K - 1) %/% 2
X <- matrix(0, nrow = n, ncol = K)
#first Fourier basis function is 1
X[,1] <- 1
col <- 2
#fills sin/cosine columns of Fourier design matrix one at a time
#J = # of pairs to include
for (k in 1:J) {
X[, col]   <- sqrt(2)*sin(2*pi*k*t)
col <- col + 1
X[, col]   <- sqrt(2)*cos(2*pi*k*t)
col <- col + 1
}
return(X)
}
#penalty matrix assuming quadtratic penalty and K ?>= 3
fourier_penalty <- function(K, order = 2) {
#number of sine/cosine pairs (subtract the intercept and divide by 2)
#if K = 3, then there is 1, K=5, then there is two, etc
J <- (K - 1) %/% 2
P <- diag(0,K)
#intercept is unpenalized
#iterate of sin/cos pairs
for (j in 1:J){
#term for each "block"
term <- (2*pi*j)^4
#sin term
#need the 2j-1 term or else the sin term goes in the cosine slot
P[1+(2*j - 1), 1+(2*j - 1)] <- term
#cosine term
#don't need the -1 addeded here
P[1+(2*j), 1+(2*j)] <- term
}
return(P)
}
#part b and c
#definding basis functions, creating design matrix and penalty matrix
K <- 5
lambda.grid <- seq(0, 0.0675, length.out = 10)
Phi <- design_matrix(t_points, K)
P <- fourier_penalty(K=K)
#the dense predictions grid
t_dense <- seq(0, 1, length.out = 5*n)
Phi_dense <- design_matrix(t_dense, K)
y_true_dense <- f1(t_dense)
#precompute
XtX <- crossprod(Phi)
Xty <- crossprod(Phi, y_values)
#estimate the function for the various values of lambda
fits <- lapply(lambda.grid, function(lam) {
A <- XtX + (lam*P)
c_hat <- solve(A, Xty)
y_hat <- as.vector(Phi_dense %*% c_hat)
data.frame(t = t_dense,
predicted = y_hat,
lambda = lam,
lambda_lab = format(lam, scientific = TRUE, digits = 2),
stringsAsFactors = FALSE)
})
#bind the rows and turns the lambda into labels via scientific notation out to 2 digits
df_pred <- bind_rows(fits) %>%
mutate(lambda_lab = factor(format(lambda, scientific = TRUE, digits = 2),
levels = unique(format(lambda.grid, scientific = TRUE, digits = 2))))
df_true   <- data.frame(t = t_dense, true = y_true_dense)
df_points <- data.frame(t = t_points, y = y_values)
estimation.plot <- ggplot() +
#draw the true function as a black line
geom_line(data = df_true, aes(x = t, y = true), linewidth = 1.2, color = "black") +
#add a seperate linetype, color for each lambda
geom_line(data = df_pred,
aes(x = t, y = predicted, color = lambda_lab, linetype = lambda_lab),
linewidth = 0.9, alpha = 0.9) +
#plot observed data points as big points, but remove aesthetics from the previous layers
geom_point(data = df_points, aes(x = t, y = y), size = 2, alpha = 0.8, inherit.aes = FALSE) +
#Name legend
scale_color_discrete(name = expression(lambda)) +
scale_linetype_discrete(name = expression(lambda)) +
#Adapt title for various Basis functions used
labs(title = sprintf("Penalized Fourier LS: True vs Predictions (K = %d)", K),
x = "t", y = "f(t)") +
#use the minimal theme with a larger text size
theme_minimal(base_size = 13) +
#make legend key wider.  Might help since we use different linetypes
theme(legend.title = element_text(),
legend.key.width = unit(1.6, "lines"))
estimation.plot
#Problem 4
#Write code to do FPCA
#read in data
prob4.dat <- readMat("Datasets/HW1/Problem 4/DataFile1_0_1.mat")
View(prob4.dat)
#Problem 4
#Write code to do FPCA
#read in data
prob4.dat.1 <- readMat("Datasets/HW1/Problem 4/DataFile1_0_1.mat")
prob4.dat.2 <- readMat("Datasets/HW1/Problem 4/DataFile1_0_2.mat")
View(prob4.dat.2)
View(prob4.dat.1)
#grid size
n <- 50
#number of curves
n <- 20
#grid size
m <- 50
#time sequence
t <- seq(0,1,lenght.out=m)
#time sequence
t <- seq(0,1,length.out=m)
phi1 <- sqrt(2)*cost(pi*t)
phi2 <- sqrt(2)*cos(3*pi*t)
plot(phi2)
#set seed
set.seed(123456)
#random coefficients
a <- rnrom(0,1)
#random coefficients
a <- rnorm(0,1)
b <- rnorm(0,1)
#random coefficients
a <- rnorm(n)
b <- rnorm(n)
F.dat <- matrix(NA, nrow = n, ncol = m)
for (i in 1:n) {
F.dat[i, ] <- a[i] * phi1 + b[i] * phi2
}
#phi functions
phi1 <- sqrt(2)*cost(pi*t)
#phi functions
phi1 <- sqrt(2)*cos(pi*t)
phi2 <- sqrt(2)*cos(3*pi*t)
#random coefficients
a <- rnorm(n)
b <- rnorm(n)
F.dat <- matrix(NA, nrow = n, ncol = m)
for (i in 1:n) {
F.dat[i, ] <- a[i] * phi1 + b[i] * phi2
}
#FPCA portion
#getting means of each column (point wise means)
mu.fun <- colMeans(F.dat)
mu.fun.mat <- matrix(mu.fun, nrow = nrow(mu.fun), ncol = ncol(mu.fun))
mu.fun.mat <- matrix(mu.fun, nrow = nrow(mu.fun), ncol = ncol(mu.fun), byrow = TRUE)
#FPCA portion
#getting means of each column (point wise means)
mu.fun <- colMeans(F.dat)
mu.fun.mat <- matrix(mu.fun, nrow = nrow(mu.fun), ncol = ncol(mu.fun), byrow = TRUE)
mu.fun.mat <- matrix(mu.fun, nrow = nrow(F.dat), ncol = ncol(F.dat))
View(mu.fun.mat)
mu.fun.mat <- matrix(mu.fun, nrow = nrow(F.dat), ncol = ncol(F.dat), byrow = TRUE)
View(mu.fun.mat)
F.dat.centered <- F.dat - mu.fun.mat
#need the covariance function
#eigen decomposition
eigen.mat <- eigen(F.dat.centered)
#Problem 6
prob6.dat <- readMat("Datasets/HW1/Problem 6/RegressionDataFile.mat")
View(prob6.dat)
str(prob6.dat)
#set up data
Fmat <- prob6.dat$f0
t <- prob6.dat$t
y <- prob6.dat$y0
plot(Fmat[1,])
library(splines)
#choose a basis.  Using B splines for this, bc we used Fourier for the previous.
#need to choose the number of basis functions
K <- 10
B <- bs(t, df=K, intercept = TRUE)
#need to discretize
dt <- t[1]
#need to discretize
dt <- t[5]
#need to discretize
dt <- t[5] - t[4]
X <- Fmat %*% (B*dt)
#fit
fit <- lm(y ~ X - 1)
summary(fit)
beta_hat <- coef(fit)
beta_hat_fun <- as.vector(B %*% beta_hat)
plot(tvec, beta_hat_fun, type = "l", main = "Estimated β(t)")
plot(t, beta_hat_fun, type = "l", main = "Estimated β(t)")
#First Homework Assignment
library(tidyverse)
library(R.matlab)
library(splines)
#Problem 6
prob6.dat <- readMat("Datasets/HW1/Problem 6/RegressionDataFile.mat")
#set up data
Fmat <- prob6.dat$f0
t <- prob6.dat$t
y <- prob6.dat$y0
#choose a basis.  Using B splines for this, bc we used Fourier for the previous.
#need to choose the number of basis functions
K <- 10
B <- bs(t, df=K, intercept = TRUE)
#need to discretize
dt <- t[5] - t[4] #evenly spaced
#projecting functions onto basis space, with the dt added in
X <- Fmat %*% (B*dt)
#fit
fit <- lm(y ~ X - 1)
summary(fit)
beta_hat <- coef(fit)
beta_hat_fun <- as.vector(B %*% beta_hat)
View(prob6.dat)
difer <- diff(t)
difer <- diff(as.vector(t))
#estimating residual variance
resid <- resid(fit)
df_res <- nrow(X) - ncol(X)
sigma2 <- sum(resid^2) / df_res
#covariance matrix of the coefficients
#sigma^2 * crossprod of X
XtX <- crossprod(X)
Vtheta <- sigma2 * solve(XtX)
BV <- B %*% Vtheta
B_var <- BV * B
B_var <- rowSums(BV*B)
#covariance matrix of the coefficients
#sigma^2 * crossprod of X
XtX <- crossprod(X)
Vtheta <- sigma2 * solve(XtX)
BV <- B %*% Vtheta
beta_var <- rowSums(BV*B)
se_beta <- sqrt(beta_var)
#critical values
crit <- qt(0.975, df = df_res)
beta_lo <- beta_hat_fun - crit * se_beta
beta_hi <- beta_hat_fun + crit * se_beta
df_plot <- data.frame(
t = as.numeric(t),
beta = beta_hat_fun,
lo = beta_lo,
hi = beta_hi
)
ggplot(df_plot, aes(x = t, y = beta)) +
geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.2, fill = "skyblue") +
geom_line(size = 1) +
labs(
title = expression(hat(beta)(t)),
y = expression(hat(beta)(t)),
x = "t"
) +
theme_minimal(base_size = 14)
ggplot(df_plot, aes(x = t, y = beta)) +
geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.2, fill = "skyblue") +
geom_line(linewidth = 1) +
labs(
title = expression(hat(beta)(t)),
y = expression(hat(beta)(t)),
x = "t"
) +
theme_minimal(base_size = 14)
#penalized regression via ridge
lambda_grid <- c(0.01, 0.1, 1, 10)
ridge_fits <- lapply(lambda_grid, function(lambda) {
theta_ridge <- solve(XtX + lambda * diag(K), crossprod(X, y))
beta_ridge <- as.vector(B %*% theta_ridge)
data.frame(t = as.numeric(t), beta = beta_ridge, lambda = lambda)
})
ridge_df <- do.call(rbind, ridge_fits)
p <- ggplot(df_plot, aes(x = t, y = beta)) +
geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.2, fill = "skyblue") +
geom_line(linewidth = 1, color = "black") +
labs(
title = expression(hat(beta)(t)),
subtitle = "Black: OLS, Colored: Ridge fits",
y = expression(hat(beta)(t)),
x = "t"
) +
theme_minimal(base_size = 14)
# Add ridge curves
p + geom_line(data = ridge_df, aes(x = t, y = beta, color = factor(lambda)),
linewidth = 1, alpha = 0.8) +
scale_color_brewer(palette = "Dark2", name = expression(lambda))
library(glmnet)
#comparing penalized regression (equivalent to ridge) to ridge
lambda_grid <- c(0.01, 0.1, 1, 10)
fit_glmnet <- glmnet(
x = X, y = y,
alpha = 0,                # ridge
lambda = lambda_grid,     # NO CV; we pass our desired lambdas
intercept = FALSE,
standardize = FALSE
)
ridge_confidence_intervals <- function(){
theta_ridge <- as.numeric(coef(fit_glmnet, s = lambda))[-1]
}
ridge_df <- do.call(rbind, lapply(lambda_grid, ridge_bands))
ridge_df <- do.call(rbind, lapply(lambda_grid, ridge_confidence_intervals))
ridge_confidence_intervals <- function(lambda){
theta_ridge <- as.numeric(coef(fit_glmnet, s = lambda))[-1]
}
#comparing penalized regression (equivalent to ridge) to ridge
lambda_grid <- c(0.01, 0.1, 1, 10)
fit_glmnet <- glmnet(
x = X, y = y,
alpha = 0,
lambda = lambda_grid,
intercept = FALSE,
standardize = FALSE
)
ridge_df <- do.call(rbind, lapply(lambda_grid, ridge_confidence_intervals))
#effective df for ridge
df_lambda <- function(lambda) sum(d / (d + lambda))
ridge_confidence_intervals <- function(lambda){
#extract the coefficients
theta_ridge <- as.numeric(coef(fit_glmnet, s = lambda))[-1]
#get the beta curve
beta_ridge <- as.vector(B %*% theta_ridge)
resid(fit_glmnet)
}
ridge_df <- do.call(rbind, lapply(lambda_grid, ridge_confidence_intervals))
ridge_confidence_intervals <- function(lambda){
#extract the coefficients
theta_ridge <- as.numeric(coef(fit_glmnet, s = lambda))[-1]
#get the beta curve
beta_ridge <- as.vector(B %*% theta_ridge)
print(resid(fit_glmnet))
}
ridge_df <- do.call(rbind, lapply(lambda_grid, ridge_confidence_intervals))
ridge_confidence_intervals <- function(lambda){
#extract the coefficients
theta_ridge <- as.numeric(coef(fit_glmnet, s = lambda))[-1]
#get the beta curve
beta_ridge <- as.vector(B %*% theta_ridge)
#need to compute residuals manually
yhat <- as.vector(X %*% theta_ridge)
rss  <- sum((y - yhat)^2)
}
ridge_df <- do.call(rbind, lapply(lambda_grid, ridge_confidence_intervals))
