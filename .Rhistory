scale_color_discrete(name = expression(lambda)) +
scale_linetype_discrete(name = expression(lambda)) +
#Adapt title for various Basis functions used
labs(title = sprintf("Penalized Fourier LS: True vs Predictions (K = %d)", K),
x = "t", y = "f(t)") +
#use the minimal theme with a larger text size
theme_minimal(base_size = 13) +
#make legend key wider.  Might help since we use different linetypes
theme(legend.title = element_text(),
legend.key.width = unit(1.6, "lines"))
estimation.plot
#part b and c
#definding basis functions, creating design matrix and penalty matrix
K <- 5
lambda.grid <- seq(0, 0.0675, length.out = 10)
Phi <- design_matrix(t_points, K)
P <- fourier_penalty(K=K)
#the dense predictions grid
t_dense <- seq(0, 1, length.out = 5*n)
Phi_dense <- design_matrix(t_dense, K)
y_true_dense <- f1(t_dense)
#precompute
XtX <- crossprod(Phi)
Xty <- crossprod(Phi, y_values)
#estimate the function for the various values of lambda
fits <- lapply(lambda.grid, function(lam) {
A <- XtX + (lam*P)
c_hat <- solve(A, Xty)
y_hat <- as.vector(Phi_dense %*% c_hat)
data.frame(t = t_dense,
predicted = y_hat,
lambda = lam,
lambda_lab = format(lam, scientific = TRUE, digits = 2),
stringsAsFactors = FALSE)
})
#bind the rows and turns the lambda into labels via scientific notation out to 2 digits
df_pred <- bind_rows(fits) %>%
mutate(lambda_lab = factor(format(lambda, scientific = TRUE, digits = 2),
levels = unique(format(lambda.grid, scientific = TRUE, digits = 2))))
df_true   <- data.frame(t = t_dense, true = y_true_dense)
df_points <- data.frame(t = t_points, y = y_values)
estimation.plot <- ggplot() +
#draw the true function as a black line
geom_line(data = df_true, aes(x = t, y = true), linewidth = 1.2, color = "black") +
#add a seperate linetype, color for each lambda
geom_line(data = df_pred,
aes(x = t, y = predicted, color = lambda_lab, linetype = lambda_lab),
linewidth = 0.9, alpha = 0.9) +
#plot observed data points as big points, but remove aesthetics from the previous layers
geom_point(data = df_points, aes(x = t, y = y), size = 2, alpha = 0.8, inherit.aes = FALSE) +
#Name legend
scale_color_discrete(name = expression(lambda)) +
scale_linetype_discrete(name = expression(lambda)) +
#Adapt title for various Basis functions used
labs(title = sprintf("Penalized Fourier LS: True vs Predictions (K = %d)", K),
x = "t", y = "f(t)") +
#use the minimal theme with a larger text size
theme_minimal(base_size = 13) +
#make legend key wider.  Might help since we use different linetypes
theme(legend.title = element_text(),
legend.key.width = unit(1.6, "lines"))
estimation.plot
View(fits)
fits[[1]]
#estimate the function for the various values of lambda
fits <- lapply(lambda.grid, function(lam) {
A <- XtX + (lam*P)
c_hat <- solve(A, Xty)
y_hat <- as.vector(Phi_dense %*% c_hat)
rss   <- sum((y_values - y_hat)^2)
data.frame(t = t_dense,
predicted = y_hat,
lambda = lam,
RSS = rss,
lambda_lab = format(lam, scientific = TRUE, digits = 2),
stringsAsFactors = FALSE)
})
#estimate the function for the various values of lambda
fits <- lapply(lambda.grid, function(lam) {
A <- XtX + (lam*P)
c_hat <- solve(A, Xty)
y_hat <- as.vector(Phi_dense %*% c_hat)
data.frame(t = t_dense,
predicted = y_hat,
lambda = lam,
lambda_lab = format(lam, scientific = TRUE, digits = 2),
stringsAsFactors = FALSE)
})
#bind the rows and turns the lambda into labels via scientific notation out to 2 digits
df_pred <- bind_rows(fits) %>%
mutate(lambda_lab = factor(format(lambda, scientific = TRUE, digits = 2),
levels = unique(format(lambda.grid, scientific = TRUE, digits = 2))))
df_true   <- data.frame(t = t_dense, true = y_true_dense)
df_points <- data.frame(t = t_points, y = y_values)
estimation.plot <- ggplot() +
#draw the true function as a black line
geom_line(data = df_true, aes(x = t, y = true), linewidth = 1.2, color = "black") +
#add a seperate linetype, color for each lambda
geom_line(data = df_pred,
aes(x = t, y = predicted, color = lambda_lab, linetype = lambda_lab),
linewidth = 0.9, alpha = 0.9) +
#plot observed data points as big points, but remove aesthetics from the previous layers
geom_point(data = df_points, aes(x = t, y = y), size = 2, alpha = 0.8, inherit.aes = FALSE) +
#Name legend
scale_color_discrete(name = expression(lambda)) +
scale_linetype_discrete(name = expression(lambda)) +
#Adapt title for various Basis functions used
labs(title = sprintf("Penalized Fourier LS: True vs Predictions (K = %d)", K),
x = "t", y = "f(t)") +
#use the minimal theme with a larger text size
theme_minimal(base_size = 13) +
#make legend key wider.  Might help since we use different line types
theme(legend.title = element_text(),
legend.key.width = unit(1.6, "lines"))
estimation.plot
#Problem 4
#Write code to do FPCA
fpca_cust <- function(Ft, t, K = 3){
n  <- nrow(Ft); m <- ncol(Ft)
#get the mean function by column
mu <- colMeans(Ft)
#plot, want plot of mean function
df_mu <- data.frame(t = as.vector(t), mu = mu)
mean_plot <- ggplot(df_mu, aes(x = t, y = mu)) +
geom_line(linewidth = 1) +
labs(title = "Mean Function", x = "t", y = "Mean f(t)") +
theme_minimal()
print(mean_plot)
#center the function
F.centered <- scale(Ft, scale=F)
#assuming evenly spaced time points
dt <- diff(as.vector(t))[1]
#estimating covariance operator
#C_hat <- crossprod(F.centered)/n
#eigen decomposition
# decomp <- eigen(C_hat, symmetric = TRUE)
# lambda <- decomp$values
# psi <- decomp$vectors
#svd decomp
n <- nrow(F.centered)
sv <- svd(F.centered/sqrt(n))
#get the principal directions
psi <- sv$v
#square the singular values to get eigenvalues
#dt bc we are approximating the inner product integral with a sum
lambda  <- (sv$d)^2 * dt
#computing coefficients
#get one score for each function
scores <- F.centered %*% (psi * dt)
#getting into the plotting mechanisms now
#plotting raw data first
df_mu <- data.frame(t = t, mu = mu)
df_raw <- data.frame(
id    = factor(rep(seq_len(n), times = m)),
t     = rep(t, each = n),
value = as.vector(Ft)
)
raw_plot <- ggplot(df_raw, aes(x = t, y = value, group = id)) +
geom_line(alpha = 0.25) +
labs(title = "Raw Functions", x = "t", y = "f(t)") +
theme_minimal()
print(raw_plot)
#getting the K dominant directions
psiK <- psi[, seq_len(K), drop = FALSE]
colnames(psiK) <- paste0("PC", seq_len(K))
df_psi <- data.frame(t = as.vector(t), psiK)
#pivotting longer for plotting
psi_long <- tidyr::pivot_longer(df_psi, -t, names_to = "Direction", values_to = "Value")
eigen.vector_plot <- ggplot(psi_long, aes(x = t, y = Value, color = Direction)) +
geom_line(linewidth = 1) +
labs(title = paste0("Leading FPCA Directions (Top ", K, ")"),
x = "t", y = "Eigenfunction Value") +
theme_minimal()
print(eigen.vector_plot)
#scree plot
df_lambda <- data.frame(Index = seq_along(lambda), Eigenvalue = lambda)
scree_plot <- ggplot(df_lambda, aes(x = Index, y = Eigenvalue)) +
geom_point(size = 2) +
geom_line() +
labs(title = "Eigenvalues (Scree Plot)", x = "Component", y = "Eigenvalue") +
theme_minimal()
print(scree_plot)
#print the variance explained by the K PCs
total_var <- sum(lambda)
pve <- lambda / total_var
cum_pve <- cumsum(pve)
cat("\nProportion of Variance Explained (PVE):\n")
cat(paste(sprintf("  PC%-2d: %6.2f%%   |  Cumulative: %6.2f%%",
1:K, 100*pve[1:K], 100*cum_pve[1:K]),
collapse = "\n"), "\n\n")
#doing the correlation plot
scores_df <- as.data.frame(scores[, seq_len(K), drop = FALSE])
colnames(scores_df) <- paste0("Coefficient:", seq_len(K))
top_coef <- ggpairs(scores_df, title = paste("Pairwise Scatter of Top", K, "FPCA Coefficients"))
print(top_coef)
#combining everything
print(raw_plot / (mean_plot + eigen.vector_plot))
}
#read in data
#dataset 1
prob4.dat.1 <- readMat("Datasets/HW1/Problem 4/DataFile1_0_1.mat")
#implementing FPCA.  Need to center the functions
Fmatrix <- prob4.dat.1$f
ts <- prob4.dat.1$t
prob4.ques1 <- fpca_cust(F = Fmatrix, t = ts)
#Problem 4
#Write code to do FPCA
fpca_cust <- function(Ft, t, K = 3){
n  <- nrow(Ft); m <- ncol(Ft)
#get the mean function by column
mu <- colMeans(Ft)
#plot, want plot of mean function
df_mu <- data.frame(t = as.vector(t), mu = mu)
mean_plot <- ggplot(df_mu, aes(x = t, y = mu)) +
geom_line(linewidth = 1) +
labs(title = "Mean Function", x = "t", y = "Mean f(t)") +
theme_minimal()
print(mean_plot)
#center the function
F.centered <- scale(Ft, scale=F)
#assuming evenly spaced time points, for later
dt <- diff(as.vector(t))[1]
#svd decomp
n <- nrow(F.centered)
sv <- svd(F.centered/sqrt(n))
#get the principal directions
psi <- sv$v
#square the singular values to get eigenvalues
#dt bc we are approximating the inner product integral with a sum
lambda  <- (sv$d)^2 * dt
#computing coefficients
#get one score for each function
scores <- F.centered %*% (psi * dt)
#getting into the plotting mechanisms now
#plotting raw data first
df_mu <- data.frame(t = t, mu = mu)
df_raw <- data.frame(
id    = factor(rep(seq_len(n), times = m)),
t     = rep(t, each = n),
value = as.vector(Ft)
)
raw_plot <- ggplot(df_raw, aes(x = t, y = value, group = id)) +
geom_line(alpha = 0.25) +
labs(title = "Raw Functions", x = "t", y = "f(t)") +
theme_minimal()
print(raw_plot)
#getting the K dominant directions
psiK <- psi[, seq_len(K), drop = FALSE]
colnames(psiK) <- paste0("PC", seq_len(K))
df_psi <- data.frame(t = as.vector(t), psiK)
#pivotting longer for plotting
psi_long <- tidyr::pivot_longer(df_psi, -t, names_to = "Direction", values_to = "Value")
eigen.vector_plot <- ggplot(psi_long, aes(x = t, y = Value, color = Direction)) +
geom_line(linewidth = 1) +
labs(title = paste0("Leading FPCA Directions (Top ", K, ")"),
x = "t", y = "Eigenfunction Value") +
theme_minimal()
print(eigen.vector_plot)
#scree plot
df_lambda <- data.frame(Index = seq_along(lambda), Eigenvalue = lambda)
scree_plot <- ggplot(df_lambda, aes(x = Index, y = Eigenvalue)) +
geom_point(size = 2) +
geom_line() +
labs(title = "Eigenvalues (Scree Plot)", x = "Component", y = "Eigenvalue") +
theme_minimal()
print(scree_plot)
#print the variance explained by the K PCs
total_var <- sum(lambda)
pve <- lambda / total_var
cum_pve <- cumsum(pve)
cat("\nProportion of Variance Explained (PVE):\n")
cat(paste(sprintf("  PC%-2d: %6.2f%%   |  Cumulative: %6.2f%%",
1:K, 100*pve[1:K], 100*cum_pve[1:K]),
collapse = "\n"), "\n\n")
#doing the correlation plot
scores_df <- as.data.frame(scores[, seq_len(K), drop = FALSE])
colnames(scores_df) <- paste0("Coefficient:", seq_len(K))
top_coef <- ggpairs(scores_df, title = paste("Pairwise Scatter of Top", K, "FPCA Coefficients"))
print(top_coef)
#combining everything
print(raw_plot / (mean_plot + eigen.vector_plot))
}
#Problem 6
prob6.dat <- readMat("Datasets/HW1/Problem 6/RegressionDataFile.mat")
#set up data
Fmat <- prob6.dat$f0
t <- as.numeric(prob6.dat$t)
y <- prob6.dat$y0
#plotting the functional data we have
df_fun <- data.frame(
t = rep(t, each = nrow(Fmat)),
curve = factor(rep(seq_len(nrow(Fmat)), times = ncol(Fmat))),
value = as.vector(Fmat)
)
ggplot(df_fun, aes(x = t, y = value, group = curve)) +
geom_line(alpha = 0.35) +
labs(title = "Original functional predictors F_i(t)",
x = "t", y = "F_i(t)") +
theme_minimal(base_size = 14)
#basis (B-splines) and design matrix
K <- 5
B  <- bs(t, df = K, intercept = TRUE)
dt <- diff(t)[1]
X  <- Fmat %*% (B*dt)
#OLS fit
fit_ols <- lm(y ~ X -1)
#basis (B-splines) and design matrix
K <- 5
B  <- bs(t, df = K, intercept = TRUE)
dt <- diff(t)[1]
X  <- Fmat %*% (B*dt)
D2 <- diff(diag(K), differences = 2)
P  <- crossprod(D2)
XtX  <- crossprod(X)
Xty  <- crossprod(X, y)
A    <- XtX + (lambda*P)
View(P)
lambda.grid <- c(0.1)
XtX  <- crossprod(X)
Xty  <- crossprod(X, y)
A    <- XtX + (lambda*P)
lambda.grid <- c(0.1)
XtX  <- crossprod(X)
Xty  <- crossprod(X, y)
A    <- XtX + (lambda.grid*P)
c_hat <- solve(A, Xty)
#hat matrices
S_lambda <- X %*% solve(A, t(X))
y_hat    <- as.vector(S_lambda %*% y)
df_eff <- sum(diag(S_lambda))
sigma2_hat <- sum((y - y_hat)^2) / (n - df_eff)
#hat matrices
S_lambda <- X %*% solve(A, t(X))
y_hat    <- as.vector(S_lambda %*% y)
df_eff <- sum(diag(S_lambda))
sigma2_hat <- sum((y - y_hat)^2) / (n - df_eff)
n <- nrow(Fmat)
#plotting the functional data we have
df_fun <- data.frame(
t = rep(t, each = nrow(Fmat)),
curve = factor(rep(seq_len(nrow(Fmat)), times = ncol(Fmat))),
value = as.vector(Fmat)
)
ggplot(df_fun, aes(x = t, y = value, group = curve)) +
geom_line(alpha = 0.35) +
labs(title = "Original functional predictors F_i(t)",
x = "t", y = "F_i(t)") +
theme_minimal(base_size = 14)
#basis (B-splines) and design matrix
K <- 5
B  <- bs(t, df = K, intercept = TRUE)
dt <- diff(t)[1]
X  <- Fmat %*% (B*dt)
#second difference penalty matrix
D2 <- diff(diag(K), differences = 2)
P  <- crossprod(D2)
lambda.grid <- c(0.1)
XtX  <- crossprod(X)
Xty  <- crossprod(X, y)
A <- XtX + (lambda.grid*P)
c_hat <- solve(A, Xty)
#hat matrices
S_lambda <- X %*% solve(A, t(X))
y_hat    <- as.vector(S_lambda %*% y)
df_eff <- sum(diag(S_lambda))
sigma2_hat <- sum((y - y_hat)^2) / (n - df_eff)
Ainv   <- solve(A)
Var_c  <- sigma2_hat * (Ainv %*% XtX %*% Ainv)
A <- crossprod(Phi) + (lambda*P)
A <- crossprod(X) + (lambda*P)
lammbda <- c(0.1)
A <- crossprod(X) + (lambda*P)
lambda <- c(0.1)
A <- crossprod(X) + (lambda*P)
S <- solve(A, t(X))
c_hat <- S %*% y
H <- X %*% S
y_hat <- as.vector(H %*% y)
df_eff <- sum(diag(H))
sigma2_hat <- sum((y - y_hat)^2) / (nrow(Fmat) - df_eff)
#variances
t_dense <- seq(min(t), max(t), length.out = 100)
B1 <- bs(t_dense, df=K, intercept = TRUE)
L_star <- B_dense %*% S
L_star <- B1 %*% S
beta_hat_dense <- as.vector(L_star %*% y)
beta_var_dense <- sigma2_hat * rowSums(L_star * L_star)   # diag(L_* L_*^T)
beta_se_dense  <- sqrt(pmax(beta_var_dense, 0))
# 95% pointwise intervals
z <- qnorm(0.975)
beta_lo <- beta_hat_dense - z * beta_se_dense
beta_hi <- beta_hat_dense + z * beta_se_dense
df_beta <- data.frame(
t    = t_dense,
beta = beta_hat_dense,
lo   = beta_lo,
hi   = beta_hi
)
ggplot(df_beta, aes(x = t, y = beta)) +
geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.18) +
geom_line(linewidth = 1) +
labs(
title = expression(hat(beta)(t) ~ "with 95% pointwise CI"),
x = "t",
y = expression(hat(beta)(t))
) +
theme_minimal(base_size = 14)
t_dense <- seq(min(t), max(t), length.out = 200)
B_dense <- bs(
t_dense, df = K, intercept = TRUE,
degree = attr(B, "degree") %||% 3,
knots = attr(B, "knots"),
Boundary.knots = attr(B, "Boundary.knots")
)
XtX <- crossprod(X)
Xty <- crossprod(X, y)
t_dense <- seq(min(t), max(t), length.out = 200)
B_dense <- bs(t_dense, df = K, intercept = TRUE)
XtX <- crossprod(X)
Xty <- crossprod(X, y)
lambda_grid <- c(0, 1e-4, 1e-3, 1e-2, 1e-1, 1)
# Helper to fit and return a data.frame for plotting
fit_one_lambda <- function(lam) {
A      <- XtX + lam * P
S      <- solve(A, t(X))                     # K x n  (y2cMap)
H      <- X %*% S                             # n x n  (y2yMap)
y_hat  <- as.vector(H %*% y)
# effective df and sigma^2
df_eff      <- sum(diag(H))
sigma2_hat  <- sum((y - y_hat)^2) / (nrow(Fmat) - df_eff)
# L_* = B_dense %*% S (maps y -> beta(t_*))
L_star <- B_dense %*% S                      # m* x n
beta_hat_dense <- as.vector(L_star %*% y)
# Var{beta_hat(t)} = sigma^2 * rowSums(L_*^2)
beta_var_dense <- sigma2_hat * rowSums(L_star * L_star)
beta_se_dense  <- sqrt(pmax(beta_var_dense, 0))
z <- qnorm(0.975)
beta_lo <- beta_hat_dense - z * beta_se_dense
beta_hi <- beta_hat_dense + z * beta_se_dense
lambda_lab <- paste0("λ=", format(lam, scientific = TRUE, digits = 2),
"\n df=", sprintf("%.1f", df_eff))
data.frame(
t    = t_dense,
beta = beta_hat_dense,
lo   = beta_lo,
hi   = beta_hi,
lambda = lam,
lambda_lab = lambda_lab,
df_eff = df_eff
)
}
# Run the sweep
df_all <- do.call(rbind, lapply(lambda_grid, fit_one_lambda))
# Facetted plot
ggplot(df_all, aes(x = t, y = beta)) +
geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.18) +
geom_line(linewidth = 1) +
facet_wrap(~ lambda_lab, ncol = 3) +
labs(
title = expression(hat(beta)(t) ~ "with 95% pointwise CI across" ~ lambda),
x = "t",
y = expression(hat(beta)(t))
) +
theme_minimal(base_size = 14)
# Helper to fit and return a data.frame for plotting
fit_one_lambda <- function(lam) {
A      <- XtX + lam * P
S      <- solve(A, t(X))                     # K x n  (y2cMap)
H      <- X %*% S                             # n x n  (y2yMap)
y_hat  <- as.vector(H %*% y)
# effective df and sigma^2
df_eff      <- sum(diag(H))
sigma2_hat  <- sum((y - y_hat)^2) / (nrow(Fmat) - df_eff)
# L_* = B_dense %*% S (maps y -> beta(t_*))
L_star <- B_dense %*% S                      # m* x n
beta_hat_dense <- as.vector(L_star %*% y)
# Var{beta_hat(t)} = sigma^2 * rowSums(L_*^2)
beta_var_dense <- sigma2_hat * rowSums(L_star * L_star)
beta_se_dense  <- sqrt(beta_var_dense)
z <- qnorm(0.975)
beta_lo <- beta_hat_dense - z * beta_se_dense
beta_hi <- beta_hat_dense + z * beta_se_dense
lambda_lab <- paste0("λ=", format(lam, scientific = TRUE, digits = 2),
"\n df=", sprintf("%.1f", df_eff))
data.frame(
t    = t_dense,
beta = beta_hat_dense,
lo   = beta_lo,
hi   = beta_hi,
lambda = lam,
lambda_lab = lambda_lab,
df_eff = df_eff
)
}
# Run the sweep
df_all <- do.call(rbind, lapply(lambda_grid, fit_one_lambda))
# Facetted plot
ggplot(df_all, aes(x = t, y = beta)) +
geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.18) +
geom_line(linewidth = 1) +
facet_wrap(~ lambda_lab, ncol = 3) +
labs(
title = expression(hat(beta)(t) ~ "with 95% pointwise CI across" ~ lambda),
x = "t",
y = expression(hat(beta)(t))
) +
theme_minimal(base_size = 14)
ggplot(df_fun, aes(x = t, y = value, group = curve)) +
geom_line(alpha = 0.35) +
labs(title = "Original functional predictors F_i(t)",
x = "t", y = "F_i(t)") +
theme_minimal(base_size = 14)
